{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc63afd",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#fff; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#5d3a8e; font-size:40px'> 3.1 Feature Extraction (Base)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89cea8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> Table of Content</h2>\n",
    "</div>\n",
    "\n",
    "* [Required Libraries and Modules](#Required-Libraries-and-Modules)\n",
    "* [Textual Features](#Textual-Features)\n",
    "* [Sentiment Features](#Sentiment-Features)\n",
    "* [Word Embeddings](#Word-Embeddings)\n",
    "* [Personality Traits](#Personality-Traits)\n",
    "* [Psycholinguistics](#Psycholinguistics)\n",
    "* [Term Lists](#Term-Lists)\n",
    "* [Combination of Features](#Combination-of-Features)\n",
    "* [Target Classes](#Target-Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84df53",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "**How can I combine different features?**\n",
    "\n",
    "Usually, if possible, you'd want to keep your matrice sparse as long as possible as it saves a lot of memory. That's why there are sparse matrices after all, otherwise, why bother? So, even if your classifier requires you to use dense input, you might want to keep the TFIDF features as sparse, and add the other features to them in a sparse format. And then only, make the matrix dense.\n",
    "\n",
    "To do that, you could use scipy.sparse.hstack. It combines two sparse matrices together by column. scipy.sparse.vstack also exists. And of course, scipy also has the non-sparse version scipy.hstack and scipy.vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be4605",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Required Libraries and Modules</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d066f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Begin Python Imports\n",
    "import datetime, warnings, scipy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Feature Extraction -  Textual Features\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a7fc1e4",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39e4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a0dee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Import Clean Text Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9784e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4988caa1cb5540a6923e75d1edec35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################\n",
    "# Note: Change the name of data set used for feature creation\n",
    "###############################################################\n",
    "task = 'bully_binary_classification'\n",
    "data_set='bully_data_clean_with_stopword'\n",
    "    \n",
    "    \n",
    "###################\n",
    "# Import Data Set #\n",
    "###################\n",
    "bully_data_cleaned = pd.read_csv(data_set+'.csv', encoding='utf8')\n",
    "bully_data_cleaned = bully_data_cleaned.drop(['ner','pos','Unnamed: 0'],axis=1)\n",
    "# Drop uninformative columns\n",
    "bully_data_cleaned = bully_data_cleaned.drop(['emails_count',\n",
    "                                              'emoji_counts',\n",
    "                                              'hashtag_count',\n",
    "                                              'mention_count',\n",
    "                                              'urls_count',\n",
    "                                              'ner_EVENT_counts',\n",
    "                                              'ner_FAC_counts', \n",
    "                                              'ner_LANGUAGE_counts',\n",
    "                                              'ner_LAW_counts', \n",
    "                                              'ner_LOC_counts', \n",
    "                                              'ner_MONEY_counts',\n",
    "                                              'ner_NORP_counts',\n",
    "                                              'ner_ORDINAL_counts', \n",
    "                                              'ner_PERCENT_counts', \n",
    "                                              'ner_PRODUCT_counts',\n",
    "                                              'ner_QUANTITY_counts', \n",
    "                                              'ner_TIME_counts', \n",
    "                                              'ner_WORK_OF_ART_counts'],axis=1)\n",
    "                                              \n",
    "bully_data_cleaned = bully_data_cleaned[~bully_data_cleaned['text_check'].isna()]\n",
    "bully_data_cleaned = bully_data_cleaned[bully_data_cleaned['text_check'] != \"\"]\n",
    "bully_data_cleaned['role'] = bully_data_cleaned['role'].progress_apply(lambda x: 'Harasser' if x == \"Bystander_assistant\" else x)\n",
    "bully_data_cleaned = bully_data_cleaned.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a515e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Textual Features</h2>\n",
    "</div>\n",
    "\n",
    "- Textual statistics\n",
    "- TFIDF\n",
    "- N-grams (unigram, bigrams, trigrams, quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d2679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Textual Features #\n",
    "####################\n",
    "\n",
    "def combine_feature_textual(df=bully_data_cleaned,\n",
    "                            textual_stats=True,\n",
    "                            pos_stats=True,\n",
    "                            ner_stats=True,\n",
    "                            tfidf=True,\n",
    "                            tfidf_ngram=(1,1),\n",
    "                            count_vectorizer_word=True,\n",
    "                            word_ngram=(1,1),\n",
    "                            count_vectorizer_char=True,\n",
    "                            char_ngram=(1,1)\n",
    "                           ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all Textual related features\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    textual_stats: boolean\n",
    "    pos_stats: boolean\n",
    "    ner_stats: boolean\n",
    "    tfidf: boolean\n",
    "    tfidf_ngram: tuple (1,1)\n",
    "    count_vectorizer_word: boolean\n",
    "    count_vectorizer_char: boolean\n",
    "    word_ngram: tuple (1,1)\n",
    "    char_ngram: tuple (1,1)\n",
    "    '''\n",
    "    \n",
    "    # Initialize empty data frame\n",
    "    X_textual_other_df = pd.DataFrame()\n",
    "    dummy = list(range(len(df)))\n",
    "    X_textual_other_df['del'] = dummy\n",
    "    \n",
    "    \n",
    "    # TF-IDF\n",
    "    def feature_text_tfidf(df,n=(1,1)):\n",
    "        tfidf = TfidfVectorizer(ngram_range=n)\n",
    "        X_tfidf = tfidf.fit_transform(df['text_check'])\n",
    "        return X_tfidf\n",
    "    \n",
    "    \n",
    "    # CountVectorizer - Word\n",
    "    def feature_text_ngram_word(df,n=(1,1)):\n",
    "        countvec_word = CountVectorizer(ngram_range=n,analyzer=\"word\")\n",
    "        X_countvec_word = countvec_word.fit_transform(df['text_check'])\n",
    "        return X_countvec_word\n",
    "\n",
    "\n",
    "    # CountVectorizer - Char\n",
    "    def feature_text_ngram_char(df,n=(1,1)):\n",
    "        countvec_char = CountVectorizer(ngram_range=n,analyzer=\"char\")\n",
    "        X_countvec_char = countvec_char.fit_transform(df['text_check'])\n",
    "        return X_countvec_char\n",
    "    \n",
    "    \n",
    "    # Text statistics\n",
    "    def feature_textual_stats(df):\n",
    "        for x in df.columns[7:16]:\n",
    "            X_textual_other_df[x] = df[x]\n",
    "\n",
    "#         for x in df.columns[21:23]:\n",
    "#             X_textual_other_df[x] = df[x]\n",
    "            \n",
    "        for x in df.columns[df.columns.isin(['emoticon_counts'])]:\n",
    "            X_textual_other_df[x] = df[x]\n",
    "\n",
    "            \n",
    "    # POS and NER count by type\n",
    "    def feature_pos_ner(df,type='ner'):\n",
    "        if type == 'ner':\n",
    "            ner_col = df.columns[df.columns.str.contains('ner')]\n",
    "            for x in ner_col:\n",
    "                X_textual_other_df[x] = df[x]\n",
    "\n",
    "        if type == 'pos':\n",
    "            pos_col = df.columns[df.columns.str.contains('pos')]\n",
    "            for x in pos_col:\n",
    "                X_textual_other_df[x] = df[x]    \n",
    "    \n",
    "    \n",
    "    # Compile textual features\n",
    "    if textual_stats:\n",
    "        print(\"Developing Textual Feature: Text Statistics\")\n",
    "        feature_textual_stats(df=df)\n",
    "        \n",
    "    if pos_stats:\n",
    "        print(\"Developing Textual Feature: POS\")\n",
    "        feature_pos_ner(df=df,type='pos')\n",
    "        \n",
    "    if ner_stats:\n",
    "        print(\"Developing Textual Feature: NER\")\n",
    "        feature_pos_ner(df=df,type='ner')\n",
    "        \n",
    "    if tfidf:\n",
    "        print(\"Developing Textual Feature: TFIDF\")\n",
    "        X_tfidf = feature_text_tfidf(df=df,n=tfidf_ngram)\n",
    "\n",
    "        \n",
    "    if count_vectorizer_word:\n",
    "        print(\"Developing Textual Feature: NGram Word\")\n",
    "        X_countvec_word = feature_text_ngram_word(df=df,n=word_ngram) \n",
    "        \n",
    "    if count_vectorizer_char:\n",
    "        print(\"Developing Textual Feature: NGram Char\")\n",
    "        X_countvec_char = feature_text_ngram_char(df=df,n=char_ngram) \n",
    "       \n",
    "    \n",
    "    # Convert to matrix form that can be feed into sklean model\n",
    "    print(\"Consolidating all Textual Feature. Done\")\n",
    "    \n",
    "    # Only Textual statistics feature\n",
    "    if (textual_stats or pos_stats or ner_stats):\n",
    "        X_textual_other_df.drop(['del'],axis=1,inplace=True)\n",
    "        X_textual_other = sparse.csr_matrix(X_textual_other_df.values)\n",
    "        \n",
    "    # Combine all selected textual features\n",
    "    if (textual_stats or pos_stats or ner_stats) & tfidf & count_vectorizer_word & count_vectorizer_char:\n",
    "        X_textual_comb = scipy.sparse.hstack((X_textual_other,X_tfidf,X_countvec_word,X_countvec_char),format='csr')\n",
    "        \n",
    "    elif (textual_stats or pos_stats or ner_stats) & tfidf & count_vectorizer_word:\n",
    "        X_textual_comb = scipy.sparse.hstack((X_textual_other,X_tfidf,X_countvec_word),format='csr')\n",
    "    \n",
    "    elif (textual_stats or pos_stats or ner_stats) & tfidf & count_vectorizer_char:\n",
    "        X_textual_comb = scipy.sparse.hstack((X_textual_other,X_tfidf,X_countvec_char),format='csr')\n",
    "    \n",
    "    elif (textual_stats or pos_stats or ner_stats) & count_vectorizer_word & count_vectorizer_char:\n",
    "        X_textual_comb = scipy.sparse.hstack((X_textual_other,X_countvec_word,X_countvec_char),format='csr')\n",
    "        \n",
    "    elif count_vectorizer_word & count_vectorizer_char:\n",
    "        X_textual_comb = scipy.sparse.hstack((X_countvec_word,X_countvec_char),format='csr')\n",
    "        \n",
    "    elif (textual_stats or pos_stats or ner_stats):\n",
    "        X_textual_comb = X_textual_other\n",
    "        \n",
    "    elif tfidf:\n",
    "        X_textual_comb = X_tfidf\n",
    "        \n",
    "    elif count_vectorizer_word:\n",
    "        X_textual_comb = X_countvec_word\n",
    "        \n",
    "    elif count_vectorizer_char:\n",
    "        X_textual_comb = X_countvec_char\n",
    "    \n",
    "    X_textual_comb=scaler.fit_transform(X_textual_comb)\n",
    "    return X_textual_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493e6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Textual Features (Textstatistics) #\n",
    "#####################################\n",
    "\n",
    "def combine_feature_textstatistics(df=bully_data_cleaned):\n",
    "    \n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all Textual related features\n",
    "    from textstatistics package\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "   \n",
    "    '''\n",
    "    \n",
    "    # Initialize empty data frame\n",
    "    X_textstatistics_df = pd.DataFrame()\n",
    "    dummy = list(range(len(df)))\n",
    "    X_textstatistics_df['del'] = dummy\n",
    "    \n",
    "    # TextStatistics attributes\n",
    "    X_textstatistics_df['ts_automated_readability_index'] = df['text_check'].progress_apply(lambda x: textstat.automated_readability_index(x))\n",
    "    X_textstatistics_df['ts_avg_character_per_word'] = df['text_check'].progress_apply(lambda x: textstat.avg_character_per_word(x))\n",
    "    X_textstatistics_df['ts_avg_letter_per_word'] = df['text_check'].progress_apply(lambda x: textstat.avg_letter_per_word(x))\n",
    "    X_textstatistics_df['ts_avg_syllables_per_word'] = df['text_check'].progress_apply(lambda x: textstat.avg_syllables_per_word(x))\n",
    "    X_textstatistics_df['ts_coleman_liau_index'] = df['text_check'].progress_apply(lambda x: textstat.coleman_liau_index(x))\n",
    "    X_textstatistics_df['ts_dale_chall'] = df['text_check'].progress_apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "    X_textstatistics_df['ts_dale_chall2'] = df['text_check'].progress_apply(lambda x: textstat.dale_chall_readability_score_v2(x))\n",
    "    X_textstatistics_df['ts_difficult_words'] = df['text_check'].progress_apply(lambda x: textstat.difficult_words(x))\n",
    "    X_textstatistics_df['ts_flesch_kincaid_grade'] = df['text_check'].progress_apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "    X_textstatistics_df['ts_flesch_reading_ease'] = df['text_check'].progress_apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "    X_textstatistics_df['ts_gunning_fog'] = df['text_check'].progress_apply(lambda x: textstat.gunning_fog(x))\n",
    "    X_textstatistics_df['ts_letter_count'] = df['text_check'].progress_apply(lambda x: textstat.letter_count(x))\n",
    "    X_textstatistics_df['ts_lexicon_count'] = df['text_check'].progress_apply(lambda x: textstat.lexicon_count(x))\n",
    "    X_textstatistics_df['ts_linsear_write_formula'] = df['text_check'].progress_apply(lambda x: textstat.linsear_write_formula(x))\n",
    "    X_textstatistics_df['ts_lix'] = df['text_check'].progress_apply(lambda x: textstat.lix(x))\n",
    "    X_textstatistics_df['ts_mcalpine_eflaw'] = df['text_check'].progress_apply(lambda x: textstat.mcalpine_eflaw(x))\n",
    "    X_textstatistics_df['ts_miniword_count'] = df['text_check'].progress_apply(lambda x: textstat.miniword_count(x))\n",
    "    X_textstatistics_df['ts_monosyllabcount'] = df['text_check'].progress_apply(lambda x: textstat.monosyllabcount(x))\n",
    "    X_textstatistics_df['ts_polysyllabcount'] = df['text_check'].progress_apply(lambda x: textstat.polysyllabcount(x))\n",
    "    X_textstatistics_df['ts_srix'] = df['text_check'].progress_apply(lambda x: textstat.rix(x))\n",
    "    X_textstatistics_df['ts_long_word_count'] = df['text_check'].progress_apply(lambda x: textstat.long_word_count(x))\n",
    "    X_textstatistics_df['ts_spache_readability'] = df['text_check'].progress_apply(lambda x: textstat.spache_readability(x))\n",
    "    X_textstatistics_df['ts_text_standard'] = df['text_check'].progress_apply(lambda x: textstat.text_standard(x,float_output=True))\n",
    "   \n",
    "    \n",
    "    # Convert to matrix form that can be feed into sklean model\n",
    "    print(\"Consolidating all Textual Feature. Done\")\n",
    "    \n",
    "    X_textstatistics_df.drop(['del'],axis=1,inplace=True)\n",
    "    X_textstatistics = sparse.csr_matrix(X_textstatistics_df.values)\n",
    "    #X_textstatistics=scaler.fit_transform(X_textstatistics)\n",
    "    \n",
    "    return X_textstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ef42a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a68b81f20354c248df2468fef2ce507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44b720d86e243f9bcb6e1633573c432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e2878bb6e1401f9a75d3dcf992608c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc0986eb35a4685a63083aea211bde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b8adf178b44dd288a97f475a29530d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0b530dfac24298859d7bcb2341af5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87110ab0b3d6442b97c5af25ba95f38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c26ee001612417fa6df5ffe197e7f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61639f9bae0e4169895f1569dffe4f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4812ce9281e940b18bae79c2539a1803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21070a006d924cf0aab621d7f7aed214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427ad4aaddc749dc8ca1d43a6a87f59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dff423474bd4940b272733cbef715b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16879a747abc4e1fabb96cff1b6e90c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ab1a1a5d8b4761ada8dea109bb3099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085567342272447f87933d0e355549c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2788ee5c424c2fbfe1c12a1095f5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8339127c184a7088075088856b3053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d6aba6116048b59d7a2b950264da34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9354d34b801c42e797d79e52d693cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2fe85c31634ee8a25cc37ac6d1e0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccf40f95ed84e499a2bf5935be08fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89c3347864742c1b124cd709f03b98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating all Textual Feature. Done\n"
     ]
    }
   ],
   "source": [
    "X_textstatistics = combine_feature_textstatistics(df=bully_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c948a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_textual_comb = combine_feature_textual(df=bully_data_cleaned,\n",
    "#                                          textual_stats=True,\n",
    "#                                          pos_stats=True,\n",
    "#                                          ner_stats=True,\n",
    "#                                          tfidf=False,\n",
    "#                                          tfidf_max_feature=100000,\n",
    "#                                          tfidf_ngram=3,\n",
    "#                                          count_vectorizer=True,\n",
    "#                                          ngram_max_feature=100000,\n",
    "#                                          ngram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c78855",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Sentiment Features</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42694c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Sentiment Features #\n",
    "######################\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pysentiment2 as ps\n",
    "from afinn import Afinn\n",
    "from nrclex import NRCLex\n",
    "\n",
    "def combine_feature_sentiment(df=bully_data_cleaned,\n",
    "                              textblob=True,\n",
    "                              vadersenti=True,\n",
    "                              pysenti=True,\n",
    "                              afinn_senti=True,\n",
    "                              nrclex=True\n",
    "                           ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all Sentiment related features\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    textblob: Boolean\n",
    "    vadersenti: Boolean\n",
    "    pysenti: Boolean\n",
    "    afinn: Boolean\n",
    "    nrclex: Boolean\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize empty data frame\n",
    "    X_senti_comb_df = pd.DataFrame()\n",
    "    dummy = list(range(len(df)))\n",
    "    X_senti_comb_df['del'] = dummy\n",
    "    \n",
    "    \n",
    "    # TextBlob: Sentiment Polarity\n",
    "    def feature_senti_polarity(x):\n",
    "        return TextBlob(x).polarity\n",
    "\n",
    "    # TextBlob: Subjectivity\n",
    "    def feature_senti_subjectivity(x):\n",
    "        return TextBlob(x).subjectivity\n",
    "    \n",
    "    # Vader Sentiment\n",
    "    # - negative score\n",
    "    # - neutral score\n",
    "    # - positive score\n",
    "    # - compound score\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    def feature_vader_senti(x):\n",
    "        return list(analyzer.polarity_scores(x).values())\n",
    "        \n",
    "    # pysentiment2\n",
    "    # Using dictionary from General Inquirer\n",
    "    # - positive score\n",
    "    # - negative score\n",
    "    # - polarity score\n",
    "    # - subjectivity score\n",
    "    hiv4 = ps.HIV4()\n",
    "    def feature_inquirer_senti(x):\n",
    "        tokens = hiv4.tokenize(x)  # text can be tokenized by other ways\n",
    "                                   # however, dict in HIV4 is preprocessed\n",
    "                                   # by the default tokenizer in the library\n",
    "        score = hiv4.get_score(tokens)\n",
    "        return list(score.values())\n",
    "    \n",
    "    # Afinn\n",
    "    # sentiment score\n",
    "    afinn = Afinn()\n",
    "    def feature_afinn_senti(x):\n",
    "        return afinn.score(x)\n",
    "    \n",
    "    # NRCLEX\n",
    "    # - fear score\n",
    "    # - anger score\n",
    "    # - anticipation score\n",
    "    # - trust score\n",
    "    # - surprise score\n",
    "    # - positive score\n",
    "    # - negative score\n",
    "    # - sadness score\n",
    "    # - disgust score\n",
    "    # - joy score\n",
    "    def feature_nrclex_senti(x,label):\n",
    "        emotion = NRCLex(x)\n",
    "        return emotion.affect_frequencies[label]\n",
    "\n",
    "    \n",
    "    # Compile sentiment features\n",
    "    if textblob:\n",
    "        print(\"Developing Sentiment Feature: TextBlob\")\n",
    "        X_senti_comb_df['tb_senti_pol'] = df['text_check'].progress_apply(lambda x: feature_senti_polarity(x))\n",
    "        X_senti_comb_df['tb_senti_sub'] = df['text_check'].progress_apply(lambda x: feature_senti_subjectivity(x))\n",
    "        \n",
    "    if vadersenti:\n",
    "        print(\"Developing Sentiment Feature: Vader\")\n",
    "        X_senti_comb_df['vader_senti_all'] = df['text_check'].progress_apply(lambda x: feature_vader_senti(x))\n",
    "        X_senti_comb_df['vader_senti_neg'] = X_senti_comb_df['vader_senti_all'].progress_apply(lambda x: x[0])\n",
    "        X_senti_comb_df['vader_senti_neu'] = X_senti_comb_df['vader_senti_all'].progress_apply(lambda x: x[1])\n",
    "        X_senti_comb_df['vader_senti_pos'] = X_senti_comb_df['vader_senti_all'].progress_apply(lambda x: x[2])\n",
    "        X_senti_comb_df['vader_senti_comp'] = X_senti_comb_df['vader_senti_all'].progress_apply(lambda x: x[3])\n",
    "        X_senti_comb_df.drop(['vader_senti_all'],axis=1,inplace=True)\n",
    "        \n",
    "    if pysenti:\n",
    "        print(\"Developing Sentiment Feature: General Inquirer\")\n",
    "        X_senti_comb_df['inquirer_senti_all'] = df['text_check'].progress_apply(lambda x: feature_inquirer_senti(x))\n",
    "        X_senti_comb_df['inquirer_senti_pos'] = X_senti_comb_df['inquirer_senti_all'].progress_apply(lambda x: x[0])\n",
    "        X_senti_comb_df['inquirer_senti_neg'] = X_senti_comb_df['inquirer_senti_all'].progress_apply(lambda x: x[1])\n",
    "        X_senti_comb_df['inquirer_senti_pol'] = X_senti_comb_df['inquirer_senti_all'].progress_apply(lambda x: x[2])\n",
    "        X_senti_comb_df['inquirer_senti_sub'] = X_senti_comb_df['inquirer_senti_all'].progress_apply(lambda x: x[3])\n",
    "        X_senti_comb_df.drop(['inquirer_senti_all'],axis=1,inplace=True)\n",
    "        \n",
    "    if afinn_senti:\n",
    "        print(\"Developing Sentiment Feature: AFINN\")\n",
    "        X_senti_comb_df['afinn_senti'] = df['text_check'].progress_apply(lambda x: feature_afinn_senti(x))\n",
    "        \n",
    "    if nrclex:\n",
    "        print(\"Developing Sentiment Feature: NRCLEX\")\n",
    "        X_senti_comb_df['nrclex_senti_fear'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='fear')) \n",
    "        X_senti_comb_df['nrclex_senti_anger'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='anger'))\n",
    "        X_senti_comb_df['nrclex_senti_anticip'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='anticip'))\n",
    "        X_senti_comb_df['nrclex_senti_trust'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='trust'))\n",
    "        X_senti_comb_df['nrclex_senti_surprise'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='surprise'))\n",
    "        X_senti_comb_df['nrclex_senti_positive'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='positive'))\n",
    "        X_senti_comb_df['nrclex_senti_negative'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='negative'))\n",
    "        X_senti_comb_df['nrclex_senti_sadness'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='sadness'))\n",
    "        X_senti_comb_df['nrclex_senti_disgust'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='disgust'))\n",
    "        X_senti_comb_df['nrclex_senti_joy'] = df['text_check'].progress_apply(lambda x: feature_nrclex_senti(x,label='joy'))\n",
    "\n",
    "    \n",
    "    # Convert to matrix form that can be feed into sklean model\n",
    "    print(\"Consolidating all Sentiment Feature. Done\")\n",
    "    X_senti_comb_df.drop(['del'],axis=1,inplace=True)\n",
    "    X_senti_comb = sparse.csr_matrix(X_senti_comb_df.values)\n",
    "    X_senti_comb=scaler.fit_transform(X_senti_comb)\n",
    "    \n",
    "    return X_senti_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3ca49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_senti_comb = combine_feature_sentiment(df=bully_data_cleaned,\n",
    "#                                           textblob=True,\n",
    "#                                           vadersenti=False,\n",
    "#                                           pysenti=False,\n",
    "#                                           afinn_senti=False,\n",
    "#                                           nrclex=False\n",
    "#                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908aac61",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Word Embeddings</h2>\n",
    "</div>\n",
    "\n",
    "- Glove\n",
    "- Word2Vec\n",
    "- fastText\n",
    "- Contextual Word Embeddings  (BERT and its variants, ELMO, nnlm)\n",
    "\n",
    "**Note:** \n",
    "1. Reshape your data using array.reshape(1, -1) if it contains a single sample\n",
    "2. Reshape your data using array.reshape(-1, 1) if your data has a single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e156d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Word Embedding (Glove, Word2Vec, fastText) #\n",
    "##############################################\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def combine_feature_embedding(df=bully_data_cleaned,\n",
    "                              glove=False,\n",
    "                              glove_corpus='glove_wikipedia',\n",
    "                              glove_word='6B',\n",
    "                              glove_dimension=100,\n",
    "                              word2vec=False,\n",
    "                              word2vec_dimension=300,\n",
    "                              fasttext=False,\n",
    "                              fasttext_dimension=300):\n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all word embedding features\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    glove: Boolean\n",
    "    glove_corpus: Specify 'glove_wikipedia','glove_twitter', 'glove_common'\n",
    "    glove_word: Specify respective number of words in dictionary\n",
    "    glove_dimension: Specify respective dimension\n",
    "    word2vec: Boolean\n",
    "    word2vec_dimension: Specify respective dimension, 300 by default\n",
    "    fasttext: Boolean\n",
    "    fasttext_dimension: Specify respective dimension, 300 by default\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ######################\n",
    "    # 1. Glove Embedding #\n",
    "    ######################\n",
    "    # Source: https://github.com/stanfordnlp/GloVe\n",
    "    def get_glove_features(df=df,\n",
    "                           corpus=glove_corpus,\n",
    "                           word=glove_word,\n",
    "                           dim=glove_dimension):\n",
    "\n",
    "        # Selection of Glove Corpus\n",
    "        path ='glove/'+ corpus + '/glove.' + word + '.' + str(dim) + 'd.txt'\n",
    "        embedding_col= corpus +'_'+ str(dim) +'_'+ 'vectors'\n",
    "\n",
    "        # Load glove vector file\n",
    "        def load_glove_vector(path):\n",
    "            glove_vectors = dict()\n",
    "            file = open(path, encoding='utf-8')\n",
    "\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "\n",
    "                word  = values[0]\n",
    "                vectors = np.asarray(values[1:])\n",
    "                glove_vectors[word] = vectors\n",
    "\n",
    "            file.close()\n",
    "            return glove_vectors\n",
    "\n",
    "        # Get Glove Vector\n",
    "        def get_glove_vec(x, glove_vectors, dim):\n",
    "            arr = np.zeros(dim)\n",
    "            text = str(x).split()\n",
    "\n",
    "            for t in text:\n",
    "                try:\n",
    "                    vec = glove_vectors.get(t).astype(float)\n",
    "                    arr = arr + vec\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            arr = arr.reshape(1, -1)[0]\n",
    "            return arr/len(text)\n",
    "\n",
    "        # Form Glove embeddings\n",
    "        def feature_glove_embedding(embedding_col,dim):\n",
    "            X = df[embedding_col].to_numpy()\n",
    "            X = X.reshape(-1, 1)\n",
    "            X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, dim)\n",
    "            X = sparse.csr_matrix(X)\n",
    "            return X\n",
    "\n",
    "        glove_vectors = load_glove_vector(path)\n",
    "        df[embedding_col] = df['text_check'].progress_apply(lambda x: get_glove_vec(x, glove_vectors, dim))\n",
    "        return feature_glove_embedding(embedding_col,dim)\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    # 2. Word2Vec Embedding #\n",
    "    #########################\n",
    "    # `spacy` Package\n",
    "    def get_word2vec_features(df=df,\n",
    "                              dim=word2vec_dimension,\n",
    "                              embedding_col='word2vec_vectors'):\n",
    "\n",
    "        def get_word2vec_vectors(x):\n",
    "            doc = nlp(x)\n",
    "            word2vec_vectors = doc.vector\n",
    "            return word2vec_vectors\n",
    "\n",
    "        def feature_word2vec_embedding(embedding_col):\n",
    "            X = df[embedding_col].to_numpy()\n",
    "            X = X.reshape(-1, 1)\n",
    "            X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, dim) #300 by default\n",
    "            X = sparse.csr_matrix(X)\n",
    "            return X\n",
    "\n",
    "        df[embedding_col] = df['text_check'].progress_apply(lambda x: get_word2vec_vectors(x))\n",
    "        return feature_word2vec_embedding(embedding_col)\n",
    "\n",
    "    \n",
    "    #########################\n",
    "    # 3. FastText Embedding #\n",
    "    #########################\n",
    "    # Source: `gensim` Package\n",
    "    # https://fasttext.cc/docs/en/english-vectors.html\n",
    "    def get_fasttext_features(df=df,\n",
    "                              dim=fasttext_dimension,\n",
    "                              embedding_col='fasttext_vectors'):\n",
    "\n",
    "        model = KeyedVectors.load_word2vec_format('fasttext/wiki-news-300d-1M.vec')\n",
    "\n",
    "       # For each input of text\n",
    "        def get_fasttext_vectors(sent, model):\n",
    "            sent_vec =[]\n",
    "            numw = 0\n",
    "\n",
    "            # store the embeddings for each word in the sentence\n",
    "            for w in sent:\n",
    "                try:\n",
    "                    if numw == 0:\n",
    "                        sent_vec = model[w]\n",
    "                    else:\n",
    "                        sent_vec = np.add(sent_vec, model[w])\n",
    "                    numw+=1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # return mean of embeddings for the sentence\n",
    "            return np.asarray(sent_vec)/numw\n",
    "\n",
    "        def feature_fasttext_embedding(embedding_col):\n",
    "            X = df[embedding_col].to_numpy()\n",
    "            X = X.reshape(-1, 1)\n",
    "            X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, dim)\n",
    "            X = sparse.csr_matrix(X)\n",
    "            return X\n",
    "\n",
    "        df[embedding_col] = df['text_check'].progress_apply(lambda x: get_fasttext_vectors(x, model))\n",
    "        return feature_fasttext_embedding(embedding_col)\n",
    "\n",
    "    \n",
    "    # Developing Embedding Vector\n",
    "    if glove:\n",
    "        print('Developing Embedding Vectors: Glove')\n",
    "        X_glove = get_glove_features(corpus=glove_corpus,word=glove_word,dim=glove_dimension)\n",
    "    \n",
    "    if word2vec:\n",
    "        print('Developing Embedding Vectors: Word2Vec')\n",
    "        X_word2vec = get_word2vec_features(embedding_col='word2vec_vectors')\n",
    "        \n",
    "    if fasttext:\n",
    "        print('Developing Embedding Vectors: FastText')\n",
    "        X_fasttext = get_fasttext_features(embedding_col='fasttext_vectors')\n",
    "\n",
    "        \n",
    "    # Combine word embeddings\n",
    "    if glove & word2vec & fasttext:\n",
    "        print('Combining all three Embedding Vectors')\n",
    "        X_embedding_comb = scipy.sparse.hstack((X_glove, X_word2vec, X_fasttext),format='csr')\n",
    "        \n",
    "    elif glove & word2vec:\n",
    "        print('Combining Glove and Word2vec Embedding Vectors')\n",
    "        X_embedding_comb = scipy.sparse.hstack((X_glove, X_word2vec),format='csr')\n",
    "\n",
    "    elif glove & fasttext:\n",
    "        print('Combining Glove and FastText Embedding Vectors')\n",
    "        X_embedding_comb = scipy.sparse.hstack((X_glove, X_fasttext),format='csr')\n",
    "        \n",
    "    elif word2vec & fasttext:\n",
    "        print('Combining Word2Vec and FastText Embedding Vectors')\n",
    "        X_embedding_comb = scipy.sparse.hstack((X_word2vec, X_fasttext),format='csr')\n",
    "    \n",
    "    elif glove:\n",
    "        print('Just Glove Embedding Vectors')\n",
    "        X_embedding_comb = X_glove\n",
    "    \n",
    "    elif word2vec:\n",
    "        print('Just Word2Vec Embedding Vectors')\n",
    "        X_embedding_comb = X_word2vec\n",
    "        \n",
    "    elif fasttext:\n",
    "        print('Just FastText Embedding Vectors')\n",
    "        X_embedding_comb = X_fasttext\n",
    "        \n",
    "    return X_embedding_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59da3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedding_comb=combine_feature_embedding(df=bully_data_cleaned,\n",
    "#                               glove=True,\n",
    "#                               glove_corpus='glove_wikipedia',\n",
    "#                               glove_word='6B',\n",
    "#                               glove_dimension=100,\n",
    "#                               word2vec=True,\n",
    "#                               word2vec_dimension=300,\n",
    "#                               fasttext=True,\n",
    "#                               fasttext_dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d545c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedding_glove = combine_feature_embedding(df=bully_data_cleaned,\n",
    "#                               glove=True,\n",
    "#                               glove_corpus='glove_wikipedia',\n",
    "#                               glove_word='6B',\n",
    "#                               glove_dimension=100\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4962bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedding_word2vec = combine_feature_embedding(df=bully_data_cleaned,\n",
    "#                                                   word2vec=True,\n",
    "#                                                   word2vec_dimension=300\n",
    "#                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f44f57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedding_fasttext = combine_feature_embedding(df=bully_data_cleaned,\n",
    "#                               fasttext=True,\n",
    "#                               fasttext_dimension=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69995bcb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Psycholinguistics</h2>\n",
    "</div>\n",
    "\n",
    "- LIWC\n",
    "- Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5517f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Import External LIWC 22 Result #\n",
    "##################################\n",
    "bully_data_liwc22 = pd.read_csv(\"LIWC22_\"+data_set+'.csv')\n",
    "bully_data_liwc22 = bully_data_liwc22[~bully_data_liwc22['text_check'].isna()]\n",
    "bully_data_liwc22 = bully_data_liwc22[bully_data_liwc22['text_check'] != \"\"]\n",
    "#bully_data_liwc = bully_data_liwc[bully_data_liwc['role']!='None']\n",
    "bully_data_liwc22 = bully_data_liwc22.drop(['Unnamed: 0','tag', 'text', 'label', 'role', 'harmfulness_score', 'oth_language',\n",
    "                                        'file_index', 'text_check', 'Segment'],axis=1)\n",
    "bully_data_liwc22 = bully_data_liwc22.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e0f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112247 entries, 0 to 112246\n",
      "Columns: 117 entries, WC to OtherP\n",
      "dtypes: float64(110), int64(7)\n",
      "memory usage: 100.2 MB\n"
     ]
    }
   ],
   "source": [
    "bully_data_liwc22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef0d8704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'BigWords',\n",
       "       'Dic', 'Linguistic', 'function',\n",
       "       ...\n",
       "       'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'QMark',\n",
       "       'Exclam', 'Apostro', 'OtherP'],\n",
       "      dtype='object', length=117)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bully_data_liwc22.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e3a768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Import External LIWC 15 Result #\n",
    "##################################\n",
    "bully_data_liwc15 = pd.read_csv(\"LIWC15_\"+data_set+'.csv')\n",
    "bully_data_liwc15 = bully_data_liwc15[~bully_data_liwc15['text_check'].isna()]\n",
    "bully_data_liwc15 = bully_data_liwc15[bully_data_liwc15['text_check'] != \"\"]\n",
    "#bully_data_liwc15 = bully_data_liwc15[bully_data_liwc15['role']!='None']\n",
    "bully_data_liwc15 = bully_data_liwc15.drop(['Unnamed: 0','tag', 'text', 'label', 'role', 'harmfulness_score', 'oth_language',\n",
    "                                        'file_index', 'text_check', 'Segment'],axis=1)\n",
    "bully_data_liwc15 = bully_data_liwc15.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "babd63d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112247 entries, 0 to 112246\n",
      "Data columns (total 93 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   WC            112247 non-null  int64  \n",
      " 1   Analytic      112247 non-null  float64\n",
      " 2   Clout         112247 non-null  float64\n",
      " 3   Authentic     112247 non-null  float64\n",
      " 4   Tone          112247 non-null  float64\n",
      " 5   WPS           112247 non-null  int64  \n",
      " 6   Sixltr        112247 non-null  float64\n",
      " 7   Dic           112247 non-null  float64\n",
      " 8   function      112247 non-null  float64\n",
      " 9   pronoun       112247 non-null  float64\n",
      " 10  ppron         112247 non-null  float64\n",
      " 11  i             112247 non-null  float64\n",
      " 12  we            112247 non-null  float64\n",
      " 13  you           112247 non-null  float64\n",
      " 14  shehe         112247 non-null  float64\n",
      " 15  they          112247 non-null  float64\n",
      " 16  ipron         112247 non-null  float64\n",
      " 17  article       112247 non-null  float64\n",
      " 18  prep          112247 non-null  float64\n",
      " 19  auxverb       112247 non-null  float64\n",
      " 20  adverb        112247 non-null  float64\n",
      " 21  conj          112247 non-null  float64\n",
      " 22  negate        112247 non-null  float64\n",
      " 23  verb          112247 non-null  float64\n",
      " 24  adj           112247 non-null  float64\n",
      " 25  compare       112247 non-null  float64\n",
      " 26  interrog      112247 non-null  float64\n",
      " 27  number        112247 non-null  float64\n",
      " 28  quant         112247 non-null  float64\n",
      " 29  affect        112247 non-null  float64\n",
      " 30  posemo        112247 non-null  float64\n",
      " 31  negemo        112247 non-null  float64\n",
      " 32  anx           112247 non-null  float64\n",
      " 33  anger         112247 non-null  float64\n",
      " 34  sad           112247 non-null  float64\n",
      " 35  social        112247 non-null  float64\n",
      " 36  family        112247 non-null  float64\n",
      " 37  friend        112247 non-null  float64\n",
      " 38  female        112247 non-null  float64\n",
      " 39  male          112247 non-null  float64\n",
      " 40  cogproc       112247 non-null  float64\n",
      " 41  insight       112247 non-null  float64\n",
      " 42  cause         112247 non-null  float64\n",
      " 43  discrep       112247 non-null  float64\n",
      " 44  tentat        112247 non-null  float64\n",
      " 45  certain       112247 non-null  float64\n",
      " 46  differ        112247 non-null  float64\n",
      " 47  percept       112247 non-null  float64\n",
      " 48  see           112247 non-null  float64\n",
      " 49  hear          112247 non-null  float64\n",
      " 50  feel          112247 non-null  float64\n",
      " 51  bio           112247 non-null  float64\n",
      " 52  body          112247 non-null  float64\n",
      " 53  health        112247 non-null  float64\n",
      " 54  sexual        112247 non-null  float64\n",
      " 55  ingest        112247 non-null  float64\n",
      " 56  drives        112247 non-null  float64\n",
      " 57  affiliation   112247 non-null  float64\n",
      " 58  achieve       112247 non-null  float64\n",
      " 59  power         112247 non-null  float64\n",
      " 60  reward        112247 non-null  float64\n",
      " 61  risk          112247 non-null  float64\n",
      " 62  focuspast     112247 non-null  float64\n",
      " 63  focuspresent  112247 non-null  float64\n",
      " 64  focusfuture   112247 non-null  float64\n",
      " 65  relativ       112247 non-null  float64\n",
      " 66  motion        112247 non-null  float64\n",
      " 67  space         112247 non-null  float64\n",
      " 68  time          112247 non-null  float64\n",
      " 69  work          112247 non-null  float64\n",
      " 70  leisure       112247 non-null  float64\n",
      " 71  home          112247 non-null  float64\n",
      " 72  money         112247 non-null  float64\n",
      " 73  relig         112247 non-null  float64\n",
      " 74  death         112247 non-null  float64\n",
      " 75  informal      112247 non-null  float64\n",
      " 76  swear         112247 non-null  float64\n",
      " 77  netspeak      112247 non-null  float64\n",
      " 78  assent        112247 non-null  float64\n",
      " 79  nonflu        112247 non-null  float64\n",
      " 80  filler        112247 non-null  float64\n",
      " 81  AllPunc       112247 non-null  float64\n",
      " 82  Period        112247 non-null  int64  \n",
      " 83  Comma         112247 non-null  int64  \n",
      " 84  Colon         112247 non-null  int64  \n",
      " 85  SemiC         112247 non-null  int64  \n",
      " 86  QMark         112247 non-null  int64  \n",
      " 87  Exclam        112247 non-null  int64  \n",
      " 88  Dash          112247 non-null  int64  \n",
      " 89  Quote         112247 non-null  int64  \n",
      " 90  Apostro       112247 non-null  float64\n",
      " 91  Parenth       112247 non-null  int64  \n",
      " 92  OtherP        112247 non-null  int64  \n",
      "dtypes: float64(81), int64(12)\n",
      "memory usage: 79.6 MB\n"
     ]
    }
   ],
   "source": [
    "bully_data_liwc15.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a85d6d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic',\n",
       "       'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they',\n",
       "       'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate',\n",
       "       'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect',\n",
       "       'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend',\n",
       "       'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat',\n",
       "       'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body',\n",
       "       'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve',\n",
       "       'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture',\n",
       "       'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home',\n",
       "       'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent',\n",
       "       'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC',\n",
       "       'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bully_data_liwc15.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d3225bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Psycholinguistics Features #\n",
    "##############################\n",
    "from empath import Empath\n",
    "\n",
    "def combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "                                      df_liwc=bully_data_liwc22,\n",
    "                                      liwc=True,\n",
    "                                      empath=True\n",
    "                           ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all Psycholinguistics related features\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    df_liwc: data frame with liwc feature name\n",
    "    liwc: Boolean\n",
    "    empath: Boolean\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize empty data frame\n",
    "    X_psycho_comb_df = pd.DataFrame()\n",
    "    dummy = list(range(len(df)))\n",
    "    X_psycho_comb_df['del'] = dummy\n",
    "    \n",
    "    \n",
    "    # LIWC 2022 Edition\n",
    "    def feature_liwc(df_liwc):\n",
    "        for x in df_liwc.columns[0:]:\n",
    "            X_psycho_comb_df['liwc_'+x] = df_liwc[x]\n",
    "            \n",
    "\n",
    "    # Empath\n",
    "    lexicon = Empath()    \n",
    "    def feature_empath(x):\n",
    "        EMPATH_CAT = lexicon.analyze(x, normalize=True)\n",
    "        return EMPATH_CAT\n",
    "\n",
    "    \n",
    "    # Compile all psycholinguistics features\n",
    "    if liwc:\n",
    "        print(\"Developing pyscholinguistics feature from liwc 2022 tools\")\n",
    "        feature_liwc(df_liwc)\n",
    "    \n",
    "    \n",
    "    if empath:\n",
    "        print(\"Developing pyscholinguistics feature from empath package\")\n",
    "        empath_analysis=df['text_check'].progress_apply(lambda x: feature_empath(x)) #saved as dictionary\n",
    "        X_empath_all = pd.DataFrame(empath_analysis.tolist())\n",
    "        X_psycho_comb_df = X_psycho_comb_df.join(X_empath_all)\n",
    "\n",
    "   \n",
    "    \n",
    "    # Convert to matrix form that can be feed into sklean model\n",
    "    print(\"Consolidating all Psycholinguistics Feature. Done\")\n",
    "    X_psycho_comb_df.drop(['del'],axis=1,inplace=True)\n",
    "    X_psycho_comb = sparse.csr_matrix(X_psycho_comb_df.values)\n",
    "    X_psycho_comb = scaler.fit_transform(X_psycho_comb)\n",
    "    \n",
    "    return X_psycho_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd02181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_psycho_comb = combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "#                                                   liwc=True,\n",
    "#                                                   empath=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6bfda",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Term Lists</h2>\n",
    "</div>\n",
    "\n",
    "Note: Binary representation feature (<numbers>: Exist, 0: Not Exist)\n",
    "- Proper names *(Already Done in NER part)*\n",
    "- 'allness' \n",
    "- absolute\n",
    "- diminishers\n",
    "- intensifiers\n",
    "- negation words\n",
    "- profane term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fcbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Term list Features #\n",
    "######################\n",
    "\n",
    "def combine_feature_termlist(df=bully_data_cleaned,\n",
    "                             absolute=False,\n",
    "                             allness=False,\n",
    "                             badword=False,\n",
    "                             negation=False,\n",
    "                             diminisher=False,\n",
    "                             intensifier=False,\n",
    "                             convert_form=\"binary\"\n",
    "                           ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to combine all Term list feature by category\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    absolute: boolean\n",
    "    allness: boolean\n",
    "    badword: boolean,\n",
    "    negation: boolean\n",
    "    diminisher: boolean\n",
    "    intensifier: boolean\n",
    "    convert_form: either \"binary\" to convert the feature to binary feature or \n",
    "                         \"ratio\" to convert the feature to term ratio \n",
    "    '''\n",
    "    \n",
    "    # Initialize empty data frame\n",
    "    X_term_comb_df = pd.DataFrame()\n",
    "\n",
    "    # Develop Term List Features\n",
    "    if convert_form == \"binary\":\n",
    "        if absolute:\n",
    "            print('Developing Term List Feature: Absolute term')\n",
    "            X_term_comb_df['term_absolute_counts']=df['term_absolute_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        if allness:\n",
    "            print('Developing Term List Feature: Allness term')\n",
    "            X_term_comb_df['term_allness_counts']=df['term_allness_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        if badword:\n",
    "            print('Developing Term List Feature: Badword term')\n",
    "            X_term_comb_df['term_badword_counts']=df['term_badword_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        if negation:\n",
    "            print('Developing Term List Feature: Negation term')\n",
    "            X_term_comb_df['term_negation_counts']=df['term_negation_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        if diminisher:\n",
    "            print('Developing Term List Feature: Diminisher term')\n",
    "            X_term_comb_df['term_diminisher_counts']=df['term_diminisher_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        if intensifier:\n",
    "            print('Developing Term List Feature: Intensifier term')\n",
    "            X_term_comb_df['term_intensifier_counts']=df['term_intensifier_counts'].progress_apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "            \n",
    "    elif convert_form == \"ratio\":\n",
    "        if absolute:\n",
    "            print('Developing Term List Feature: Absolute term')\n",
    "            X_term_comb_df['term_absolute_ratio']=(df['term_absolute_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "        if allness:\n",
    "            print('Developing Term List Feature: Allness term')\n",
    "            X_term_comb_df['term_allness_ratio']=(df['term_allness_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "        if badword:\n",
    "            print('Developing Term List Feature: Badword term')\n",
    "            X_term_comb_df['term_badword_ratio']=(df['term_badword_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "        if negation:\n",
    "            print('Developing Term List Feature: Negation term')\n",
    "            X_term_comb_df['term_negation_ratio']=(df['term_negation_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "        if diminisher:\n",
    "            print('Developing Term List Feature: Diminisher term')\n",
    "            X_term_comb_df['term_diminisher_ratio']=(df['term_diminisher_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "        if intensifier:\n",
    "            print('Developing Term List Feature: Intensifier term')\n",
    "            X_term_comb_df['term_intensifier_ratio']=(df['term_intensifier_counts']/df['word_count']*100).round(2)\n",
    "\n",
    "    elif convert_form == \"count\":\n",
    "        if absolute:\n",
    "            print('Developing Term List Feature: Absolute term')\n",
    "            X_term_comb_df['term_absolute_ratio']=df['term_absolute_counts']\n",
    "\n",
    "        if allness:\n",
    "            print('Developing Term List Feature: Allness term')\n",
    "            X_term_comb_df['term_allness_ratio']=df['term_allness_counts']\n",
    "\n",
    "        if badword:\n",
    "            print('Developing Term List Feature: Badword term')\n",
    "            X_term_comb_df['term_badword_ratio']=df['term_badword_counts']\n",
    "\n",
    "        if negation:\n",
    "            print('Developing Term List Feature: Negation term')\n",
    "            X_term_comb_df['term_negation_ratio']=df['term_negation_counts']\n",
    "\n",
    "        if diminisher:\n",
    "            print('Developing Term List Feature: Diminisher term')\n",
    "            X_term_comb_df['term_diminisher_ratio']=df['term_diminisher_counts']\n",
    "\n",
    "        if intensifier:\n",
    "            print('Developing Term List Feature: Intensifier term')\n",
    "            X_term_comb_df['term_intensifier_ratio']=df['term_intensifier_counts']\n",
    "\n",
    "            \n",
    "    # Convert to matrix form that can be feed into sklean model\n",
    "    print(\"Consolidating all Term List Feature. Done\")\n",
    "    X_term_comb = sparse.csr_matrix(X_term_comb_df.values) \n",
    "    X_term_comb = scaler.fit_transform(X_term_comb)\n",
    "    \n",
    "    return X_term_comb\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb5d70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_term_comb = combine_feature_termlist(df=bully_data_cleaned,\n",
    "#                              absolute=True,\n",
    "#                              allness=True,\n",
    "#                              badword=True,\n",
    "#                              negation=True,\n",
    "#                              diminisher=True,\n",
    "#                              intensifier=True\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a874cfd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Combination of Features</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119f0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TextStat+CountVecWordChar with unigram+bigram+trigram features\n",
      "Developing Textual Feature: Text Statistics\n",
      "Developing Textual Feature: POS\n",
      "Developing Textual Feature: NER\n",
      "Developing Textual Feature: NGram Word\n",
      "Developing Textual Feature: NGram Char\n",
      "Consolidating all Textual Feature. Done\n",
      "\n",
      "Generating SentimentAll features\n",
      "Developing Sentiment Feature: TextBlob\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69db34237bd1472aa1ea2cda7e3787be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6925b9112d14457ba967da38ac0985b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Sentiment Feature: Vader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee22106370ca4876acca9f490becdda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b08dc82cac4c6baec9b26aab9cb494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1476581693cd4c7e97a016a9eb9243cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4281c2bdb0842338753de1a42191240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06bc53f86544ca3ae7459cf76cf9807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Sentiment Feature: General Inquirer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d5806fdc07422481e4852eb260527a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4fae0f19c34c1e91b014b191e55936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ee7833533544309216dc15fc1135f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca25a5ec98a44c85a664deb792685bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cabf2d551324fbdb7c5d0daf2c98c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Sentiment Feature: AFINN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f44434b94dd4cc9880e309e1d93e4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Sentiment Feature: NRCLEX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3b54dc0884207aa1b69f1b1ddd1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242e387178f143068f237f92640fa0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45fd77a866b4dc3a1e444015bcb1093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac1cde766b04cafb5faa15a4b06d3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6218918600f341ae9f9fd627040fe434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19450fbda164c47916229a1d3995497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72e256c9fca4b5d84ee7f7b7899a5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b197a30e2c7a45a49b82f74d97eca93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783db6dc57f541d291f68420527c1361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03881347b3b941239c40278c5e3a4213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating all Sentiment Feature. Done\n",
      "\n",
      "Generating PycholinguisticLIWC22Empath features\n",
      "Developing pyscholinguistics feature from liwc 2022 tools\n",
      "Developing pyscholinguistics feature from empath package\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea40e36c4d9c412c8955669fc060c612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating all Psycholinguistics Feature. Done\n",
      "\n",
      "Generating TermListsRatio features\n",
      "Developing Term List Feature: Absolute term\n",
      "Developing Term List Feature: Allness term\n",
      "Developing Term List Feature: Badword term\n",
      "Developing Term List Feature: Negation term\n",
      "Developing Term List Feature: Diminisher term\n",
      "Developing Term List Feature: Intensifier term\n",
      "Consolidating all Term List Feature. Done\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Textual Only #\n",
    "################\n",
    "# print(\"Generating CountVecWord with unigram+bigram+trigram features\")\n",
    "# X_CountVecWordAll = combine_feature_textual(df=bully_data_cleaned,\n",
    "#                                                     textual_stats=False,\n",
    "#                                                     pos_stats=False,\n",
    "#                                                     ner_stats=False,\n",
    "#                                                     tfidf=False,\n",
    "#                                                     tfidf_ngram=(1,1),\n",
    "#                                                     count_vectorizer_word=True,\n",
    "#                                                     word_ngram=(1,3),\n",
    "#                                                     count_vectorizer_char=False,\n",
    "#                                                     char_ngram=(1,3)\n",
    "#                                                    )\n",
    "\n",
    "\n",
    "# print(\"Generating CountVecChar with unigram+bigram+trigram features\")\n",
    "# X_CountVecCharAll = combine_feature_textual(df=bully_data_cleaned,\n",
    "#                                                     textual_stats=False,\n",
    "#                                                     pos_stats=False,\n",
    "#                                                     ner_stats=False,\n",
    "#                                                     tfidf=False,\n",
    "#                                                     tfidf_ngram=(1,1),\n",
    "#                                                     count_vectorizer_word=False,\n",
    "#                                                     word_ngram=(1,3),\n",
    "#                                                     count_vectorizer_char=True,\n",
    "#                                                     char_ngram=(1,3)\n",
    "#                                                    )\n",
    "\n",
    "\n",
    "# print(\"Generating CountVecWordChar with unigram+bigram+trigram features\")\n",
    "# X_CountVecWordCharAll = combine_feature_textual(df=bully_data_cleaned,\n",
    "#                                                     textual_stats=False,\n",
    "#                                                     pos_stats=False,\n",
    "#                                                     ner_stats=False,\n",
    "#                                                     tfidf=False,\n",
    "#                                                     tfidf_ngram=(1,1),\n",
    "#                                                     count_vectorizer_word=True,\n",
    "#                                                     word_ngram=(1,3),\n",
    "#                                                     count_vectorizer_char=True,\n",
    "#                                                     char_ngram=(1,3)\n",
    "#                                                    )\n",
    "\n",
    "\n",
    "print(\"Generating TextStat+CountVecWordChar with unigram+bigram+trigram features\")\n",
    "X_CountVecWordCharAllTextStat = combine_feature_textual(df=bully_data_cleaned,\n",
    "                                                            textual_stats=True,\n",
    "                                                            pos_stats=True,\n",
    "                                                            ner_stats=True,\n",
    "                                                            tfidf=False,\n",
    "                                                            tfidf_ngram=(1,1),\n",
    "                                                            count_vectorizer_word=True,\n",
    "                                                            word_ngram=(1,3),\n",
    "                                                            count_vectorizer_char=True,\n",
    "                                                            char_ngram=(1,3)\n",
    "                                                           )\n",
    "\n",
    "\n",
    "##################\n",
    "# Sentiment Only #\n",
    "##################\n",
    "print()\n",
    "print(\"Generating SentimentAll features\")\n",
    "X_SentimentAll = combine_feature_sentiment(df=bully_data_cleaned,\n",
    "                                              textblob=True,\n",
    "                                              vadersenti=True,\n",
    "                                              pysenti=True,\n",
    "                                              afinn_senti=True,\n",
    "                                              nrclex=True)\n",
    "\n",
    "\n",
    "#######################\n",
    "# Word Embedding Only #\n",
    "#######################\n",
    "print()\n",
    "print(\"Generating GloveEmbedding features\")\n",
    "X_GloveEmbedding = combine_feature_embedding(df=bully_data_cleaned,\n",
    "                                                glove=True,\n",
    "                                                glove_corpus='glove_wikipedia',\n",
    "                                                glove_word='6B',\n",
    "                                                glove_dimension=100\n",
    "                                                )\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Generating Word2VecEmbedding features\")\n",
    "X_Word2VecEmbedding = combine_feature_embedding(df=bully_data_cleaned,\n",
    "                                                  word2vec=True,\n",
    "                                                  word2vec_dimension=300\n",
    "                                                  )\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Generating FastTextEmbedding features\")\n",
    "X_FastTextEmbedding = combine_feature_embedding(df=bully_data_cleaned,\n",
    "                                                  fasttext=True,\n",
    "                                                  fasttext_dimension=300)\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "# Psycholinguistics Only #\n",
    "##########################\n",
    "# print()\n",
    "# print(\"Generating PycholinguisticEmpath features\")\n",
    "# X_PycholinguisticEmpath = combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "#                                                             df_liwc=bully_data_liwc22,\n",
    "#                                                                 liwc=False,\n",
    "#                                                                 empath=True)\n",
    "\n",
    "\n",
    "# print()\n",
    "# print(\"Generating PycholinguisticLIWC22 features\")\n",
    "# X_PycholinguisticLIWC22 = combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "#                                                           df_liwc=bully_data_liwc22,\n",
    "#                                                               liwc=True,\n",
    "#                                                               empath=False)\n",
    "\n",
    "# print()\n",
    "# print(\"Generating PycholinguisticLIWC15 features\")\n",
    "# X_PycholinguisticLIWC15 = combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "#                                                           df_liwc=bully_data_liwc15,\n",
    "#                                                               liwc=True,\n",
    "#                                                               empath=False)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Generating PycholinguisticLIWC22Empath features\")\n",
    "X_PycholinguisticLIWC22Empath = combine_feature_psycholinguistics(df=bully_data_cleaned,\n",
    "                                                      df_liwc=bully_data_liwc22,\n",
    "                                                          liwc=True,\n",
    "                                                          empath=True)\n",
    "\n",
    "\n",
    "##################\n",
    "# Term List only #\n",
    "##################\n",
    "print()\n",
    "print(\"Generating TermListsRatio features\")\n",
    "X_TermListsRatio = combine_feature_termlist(df=bully_data_cleaned,\n",
    "                                         absolute=True,\n",
    "                                         allness=True,\n",
    "                                         badword=True,\n",
    "                                         negation=True,\n",
    "                                         diminisher=True,\n",
    "                                         intensifier=True,\n",
    "                                         convert_form = \"ratio\"\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e911d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Output as pickle files #\n",
    "##########################\n",
    "\n",
    "# Feature sets #\n",
    "feature_set = { \n",
    "               'X_CountVecWordCharAllTextStat': X_CountVecWordCharAllTextStat,\n",
    "               'X_SentimentAll': X_SentimentAll,\n",
    "                'X_GloveEmbedding': X_GloveEmbedding,\n",
    "                'X_Word2VecEmbedding': X_Word2VecEmbedding,\n",
    "                'X_FastTextEmbedding': X_FastTextEmbedding,\n",
    "                'X_PycholinguisticLIWC22Empath': X_PycholinguisticLIWC22Empath,\n",
    "                'X_TermListsRatio': X_TermListsRatio\n",
    "    \n",
    "              }\n",
    "\n",
    "for fname, fset in feature_set.items():\n",
    "    with open(task+\"\\\\\"+data_set+\"\\\\features\\\\selected_scale\\\\\"+ fname + \".pkl\",'wb') as f:\n",
    "        pickle.dump(fset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4283b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_CountVecWordCharAllTextStat.shape)\n",
    "print(X_SentimentAll.shape)\n",
    "print(X_GloveEmbedding.shape)\n",
    "print(X_Word2VecEmbedding.shape)\n",
    "print(X_FastTextEmbedding.shape)\n",
    "print(X_PycholinguisticLIWC22Empath.shape)\n",
    "print(X_TermListsRatio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e4399",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Target Classes</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ae14db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable - Cyberbullying vs Non-Cyberbullying #\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\target_class\\\\Y_cyberbullying.pkl\",'wb') as f:\n",
    "    pickle.dump(bully_data_cleaned['label'].values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f379ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_role_label(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == \"Harasser\":\n",
    "        return 1\n",
    "    elif x == \"Victim\":\n",
    "        return 2\n",
    "    elif x == \"Bystander_defender\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a620b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39f36d9fe9f496d971f9d967e42affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bully_data_cleaned['role_id'] = bully_data_cleaned['role'].progress_apply(lambda x: convert_role_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e3f333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    106872\n",
       "1      3596\n",
       "2      1354\n",
       "3       425\n",
       "Name: role_id, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bully_data_cleaned['role_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86d307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable - Cyberbullying vs Non-Cyberbullying #\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\target_class\\\\Y_role.pkl\",'wb') as f:\n",
    "    pickle.dump(bully_data_cleaned['role_id'].values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46b5b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable - Cyberbullying vs Non-Cyberbullying #\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\target_class\\\\Y_cyberbullying.pkl\",'wb') as f:\n",
    "    pickle.dump(bully_data_cleaned['label'].values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17a36b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_role_label(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == \"Harasser\":\n",
    "        return 1\n",
    "    elif x == \"Victim\":\n",
    "        return 2\n",
    "    elif x == \"Bystander_defender\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e718ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39f36d9fe9f496d971f9d967e42affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bully_data_cleaned['role_id'] = bully_data_cleaned['role'].progress_apply(lambda x: convert_role_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4baf705b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    106872\n",
       "1      3596\n",
       "2      1354\n",
       "3       425\n",
       "Name: role_id, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bully_data_cleaned['role_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17213f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable - Cyberbullying vs Non-Cyberbullying #\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\target_class\\\\Y_role.pkl\",'wb') as f:\n",
    "    pickle.dump(bully_data_cleaned['role_id'].values, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
