{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dbc463",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#fff; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#5d3a8e; font-size:40px'> 3.4 Feature Extraction (Tensorflow)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f7c9c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> Table of Content</h2>\n",
    "</div>\n",
    "\n",
    "* [Required Libraries and Modules](#Required-Libraries-and-Modules)\n",
    "* [Import Clean Text Data](#Import-Clean-Text-Data)\n",
    "* [BERT and its variants Word Embeddings TensorFlow](#BERT-and-its-variants-Word-Embeddings-TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4b73d",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "**How can I combine different features?**\n",
    "\n",
    "Usually, if possible, you'd want to keep your matrice sparse as long as possible as it saves a lot of memory. That's why there are sparse matrices after all, otherwise, why bother? So, even if your classifier requires you to use dense input, you might want to keep the TFIDF features as sparse, and add the other features to them in a sparse format. And then only, make the matrix dense.\n",
    "\n",
    "To do that, you could use scipy.sparse.hstack. It combines two sparse matrices together by column. scipy.sparse.vstack also exists. And of course, scipy also has the non-sparse version scipy.hstack and scipy.vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed65ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Required Libraries and Modules</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ed739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Begin Python Imports\n",
    "import datetime, warnings, scipy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Feature Extraction -  Textual Features\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b232ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Import Clean Text Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4949d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Note: Change the name of data set used for feature creation\n",
    "###############################################################\n",
    "task = 'bully_binary_classification'\n",
    "data_set='bully_data_clean_no_stopword_pronoun'\n",
    "    \n",
    "    \n",
    "###################\n",
    "# Import Data Set #\n",
    "###################\n",
    "bully_data_cleaned = pd.read_csv(data_set+'.csv', encoding='utf8')\n",
    "bully_data_cleaned = bully_data_cleaned.drop(['ner','pos','Unnamed: 0'],axis=1)\n",
    "bully_data_cleaned = bully_data_cleaned[~bully_data_cleaned['text_check'].isna()]\n",
    "bully_data_cleaned = bully_data_cleaned[bully_data_cleaned['text_check'] != \"\"]\n",
    "bully_data_cleaned = bully_data_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77109cc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>BERT and its variants Word Embeddings TensorFlow</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa1dc5",
   "metadata": {},
   "source": [
    "- bert\n",
    "- electra\n",
    "- albert\n",
    "- tnbert\n",
    "- ggelubert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Word Embedding (BERT and its variants) Tensorflow Hub #\n",
    "#########################################################\n",
    "\n",
    "def get_bert_features(df=bully_data_cleaned,\n",
    "                  type=\"bert\",\n",
    "                  chunk_size=100,\n",
    "                  preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                  encoder_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"):\n",
    "    \n",
    "    '''\n",
    "    -------------\n",
    "     Description \n",
    "    -------------\n",
    "    Umbrella Function to extract bert and its variants word embedding features\n",
    "    from TensorFlowHub\n",
    "    \n",
    "    ------------\n",
    "     Parameters\n",
    "    ------------\n",
    "\n",
    "    df: data frame name\n",
    "    type: specify the type of bert or its variants\n",
    "    - bert\n",
    "    - electra\n",
    "    - albert\n",
    "    - tnbert\n",
    "    - ggelubert\n",
    "    \n",
    "    chunk_size: size of chunk sets\n",
    "    preprocess_url: Refer TensorFlowHub\n",
    "    encoder_url: Refer TensorFlowHub\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def get_sentence_embeding(sentences):\n",
    "        preprocessed_text = bert_preprocess(sentences)\n",
    "        return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "    \n",
    "    # Dimension for each bert variants\n",
    "    if type == \"mobilebert\":\n",
    "        dim=512\n",
    "    elif type == \"lambert\" or type =='roberta':\n",
    "        dim=1024\n",
    "    else:\n",
    "        dim=768\n",
    "\n",
    "        \n",
    "    bert_preprocess = hub.KerasLayer(preprocess_url)\n",
    "    bert_encoder = hub.KerasLayer(encoder_url)\n",
    "    bert_embedding=np.empty((0, dim), float)\n",
    "\n",
    "    \n",
    "    for i in tqdm(np.arange(0,df.shape[0],chunk_size)):\n",
    "        temp_list = df['text_check'][i:i+chunk_size].to_list()\n",
    "        bert_vec_tf = get_sentence_embeding(temp_list)\n",
    "        bert_vec = bert_vec_tf.numpy()\n",
    "        bert_embedding = np.vstack((bert_embedding,bert_vec)) \n",
    "        \n",
    "    bert_embedding_sparse = sparse.csr_matrix(bert_embedding)\n",
    "    return bert_embedding_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 55 minutes\n",
    "# X_bert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                   type=\"bert\",\n",
    "#                                   chunk_size=100,\n",
    "#                                      preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                      encoder_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ff573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 1 hour\n",
    "# X_electra_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                      type=\"electra\",\n",
    "#                                      chunk_size=100,\n",
    "#                                          preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                          encoder_url=\"https://tfhub.dev/google/electra_base/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052df35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 50 minutes\n",
    "# X_albert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                     type=\"albert\",\n",
    "#                                     chunk_size=100,\n",
    "#                                         preprocess_url=\"http://tfhub.dev/tensorflow/albert_en_preprocess/3\",\n",
    "#                                          encoder_url=\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 1 hour\n",
    "# X_tnbert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                     type=\"tnbert\",\n",
    "#                                     chunk_size=100,\n",
    "#                                         preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                         encoder_url=\"https://tfhub.dev/google/tn_bert/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45102019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 1 hour\n",
    "# X_ggelubert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                        type=\"ggelubert\",\n",
    "#                                        chunk_size=100,\n",
    "#                                         preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                         encoder_url=\"https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_lambert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                      type=\"lambert\",\n",
    "#                                      chunk_size=10,\n",
    "#                                         preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                         encoder_url=\"https://tfhub.dev/tensorflow/lambert_en_uncased_L-24_H-1024_A-16/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee63d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chnage to huggingface\n",
    "# X_distilbert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                         type=\"distilbert\",\n",
    "#                                         chunk_size=10,\n",
    "#                                         preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                         encoder_url=\"https://tfhub.dev/jeongukjae/distilbert_en_uncased_L-6_H-768_A-12/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chnage hugging face\n",
    "# X_mobilebert_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                         type=\"mobilebert\",\n",
    "#                                         chunk_size=10,\n",
    "#                                             preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "#                                             encoder_url=\"https://tfhub.dev/tensorflow/mobilebert_en_uncased_L-24_H-128_B-512_A-4_F-4_OPT/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_roberta_features=get_bert_features(df=bully_data_cleaned,\n",
    "#                                         type=\"roberta\",\n",
    "#                                         chunk_size=1,\n",
    "#                                         preprocess_url=\"https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1\",\n",
    "#                                         encoder_url=\"https://tfhub.dev/jeongukjae/roberta_en_cased_L-24_H-1024_A-16/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30730edd",
   "metadata": {},
   "source": [
    "### ELMO Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_features(df=bully_data_cleaned):\n",
    "    \n",
    "    '''\n",
    "    -------------\n",
    "    Description \n",
    "    -------------\n",
    "    Umbrella Function to extract ELMO word embedding features\n",
    "\n",
    "    ------------\n",
    "    Parameters\n",
    "    ------------\n",
    "\n",
    "    df: specify data frame which will be used as follow with text column and convert to list\n",
    "    Eg. df['Message'].to_list()\n",
    "\n",
    "    '''\n",
    "    elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "\n",
    "    chunk_size=5\n",
    "    dim = 1024 #elmo vector dimension\n",
    "\n",
    "    elmo_embedding=np.empty((0, dim), float)\n",
    "   \n",
    "    for i in tqdm(np.arange(0,len(df['text_check'].to_list()),chunk_size)):\n",
    "        temp_list = tf.cast(df['text_check'].to_list()[i:i+chunk_size],tf.string)\n",
    "        embed = elmo_model.signatures['default'](temp_list)['elmo']\n",
    "        \n",
    "        # return average of ELMo features\n",
    "        embed_mean = tf.reduce_mean(embed,1).numpy()\n",
    "        elmo_embedding = np.vstack((elmo_embedding,embed_mean))\n",
    "        \n",
    "    elmo_embedding_sparse = sparse.csr_matrix(elmo_embedding)\n",
    "    return elmo_embedding_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 45 minutes\n",
    "# X_elmo_features = get_elmo_features(df=bully_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165185d9",
   "metadata": {},
   "source": [
    "### nnlm Word Embeddings\n",
    "```python\n",
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples[:3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nnlm_features(df=bully_data_cleaned):\n",
    "\n",
    "    nnlm_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "    dimension = 128\n",
    "\n",
    "    def transform_features(X,dimension):\n",
    "        features = np.empty((0, dimension), float)\n",
    "\n",
    "        for x in tqdm(X):\n",
    "            features = np.append(features, x.numpy(), 0)\n",
    "        return features\n",
    "\n",
    "    X = df['text_check'].progress_apply(lambda x: nnlm_model([x]))\n",
    "    X_nnlm_features=transform_features(X=X,dimension=dimension)\n",
    "    X_nnlm_features_sparse = sparse.csr_matrix(X_nnlm_features)\n",
    "    \n",
    "    return X_nnlm_features_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ff969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within 30 minutes\n",
    "# X_nnlm_features = get_nnlm_features(df=bully_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fee5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Combination of Features</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63e0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "# Bert #\n",
    "########\n",
    "print(\"Generating bert features\")\n",
    "# About 1 hour\n",
    "X_BertEmbedding=get_bert_features(df=bully_data_cleaned,\n",
    "                                  type=\"bert\",\n",
    "                                  chunk_size=100,\n",
    "                                     preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                                     encoder_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "print(\"Shape: \"+str(X_BertEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_BertEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_BertEmbedding, f)\n",
    "    \n",
    "del X_BertEmbedding # to free up memory\n",
    "gc.collect()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25520d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########\n",
    "# albert #\n",
    "##########\n",
    "print()\n",
    "print(\"Generating albert features\")\n",
    "# About 1 hour\n",
    "X_AlbertEmbedding=get_bert_features(df=bully_data_cleaned,\n",
    "                                    type=\"albert\",\n",
    "                                    chunk_size=100,\n",
    "                                        preprocess_url=\"http://tfhub.dev/tensorflow/albert_en_preprocess/3\",\n",
    "                                         encoder_url=\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n",
    "\n",
    "print(\"Shape: \"+str(X_AlbertEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_AlbertEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_AlbertEmbedding, f)\n",
    "    \n",
    "del X_AlbertEmbedding # to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb92a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# electra #\n",
    "###########\n",
    "print()\n",
    "print(\"Generating electra features\")\n",
    "# About 46 minutes\n",
    "X_ElectraEmbedding=get_bert_features(df=bully_data_cleaned,\n",
    "                                     type=\"electra\",\n",
    "                                     chunk_size=100,\n",
    "                                         preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                                         encoder_url=\"https://tfhub.dev/google/electra_base/2\")\n",
    "\n",
    "print(\"Shape: \"+str(X_ElectraEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_ElectraEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_ElectraEmbedding, f)\n",
    "    \n",
    "del X_ElectraEmbedding # to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0584ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# tnbert #\n",
    "##########\n",
    "print()\n",
    "print(\"Generating tnbert features\")\n",
    "# About 1 hour\n",
    "X_tnBertEmbedding=get_bert_features(df=bully_data_cleaned,\n",
    "                                    type=\"tnbert\",\n",
    "                                    chunk_size=80,\n",
    "                                        preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                                        encoder_url=\"https://tfhub.dev/google/tn_bert/1\")\n",
    "\n",
    "print(\"Shape: \"+ str(X_tnBertEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_tnBertEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_tnBertEmbedding, f)\n",
    "    \n",
    "del X_tnBertEmbedding # to free up memory\n",
    "gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# ggelubert #\n",
    "#############\n",
    "print()\n",
    "print(\"Generating ggelubert features\")\n",
    "# About 1 hour\n",
    "X_ggeluBertEmbedding=get_bert_features(df=bully_data_cleaned,\n",
    "                                       type=\"ggelubert\",\n",
    "                                       chunk_size=100,\n",
    "                                        preprocess_url=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                                        encoder_url=\"https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/2\")\n",
    "\n",
    "print(\"Shape: \"+str(X_ggeluBertEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_ggeluBertEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_ggeluBertEmbedding, f)\n",
    "    \n",
    "del X_ggeluBertEmbedding # to free up memory\n",
    "gc.collect()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad730289",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "########\n",
    "# elmo #\n",
    "########\n",
    "print()\n",
    "print(\"Generating elmo features\")\n",
    "# About 45 minutes\n",
    "X_elmoEmbedding = get_elmo_features(df=bully_data_cleaned)\n",
    "\n",
    "print(\"Shape: \"+str(X_elmoEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_elmoEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_elmoEmbedding, f)\n",
    "    \n",
    "del X_elmoEmbedding # to free up memory\n",
    "gc.collect()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# nnlm #\n",
    "########\n",
    "print()\n",
    "print(\"Generating nnlm features\")\n",
    "# Within 30 minutes\n",
    "X_nnlmEmbedding = get_nnlm_features(df=bully_data_cleaned)\n",
    "\n",
    "print(\"Shape: \"+str(X_nnlmEmbedding.shape)) # check shape\n",
    "\n",
    "with open(task+\"\\\\\"+data_set+\"\\\\features\\\\X_nnlmEmbedding.pkl\",'wb') as f:\n",
    "    pickle.dump(X_nnlmEmbedding, f)\n",
    "    \n",
    "del X_nnlmEmbedding # to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9558675",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Output as pickle files #\n",
    "##########################\n",
    "\n",
    "# Feature sets #\n",
    "# feature_set = { \n",
    "#                 'X_BertEmbedding': X_BertEmbedding,\n",
    "#                 'X_ElectraEmbedding': X_ElectraEmbedding,\n",
    "#                 'X_AlbertEmbedding': X_AlbertEmbedding\n",
    "#                 'X_tnBertEmbedding': X_tnBertEmbedding,\n",
    "#                 'X_ggeluBertEmbedding': X_ggeluBertEmbedding,\n",
    "#                 'X_elmo_features': X_elmo_features,\n",
    "#                 'X_nnlm_features': X_nnlm_features\n",
    "#              }\n",
    "\n",
    "# for fname, fset in feature_set.items():\n",
    "#     with open(data_set+\"\\\\features\\\\\"+ fname + \".pkl\",'wb') as f:\n",
    "#         pickle.dump(fset, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
