{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d1fce0",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#fff; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#88d8b0; font-size:40px'>5.1 Classification Model - Cyberbullying vs Not Cyberbullying</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef863c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#88d8b0; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Import Libraries or Modules </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089d831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Begin Python Imports\n",
    "import datetime, warnings, scipy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    GridSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cd4d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#88d8b0; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Train and Test Classifier</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec783f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "svc = LinearSVC(random_state=1127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea45be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Train and Test Classifiers #\n",
    "##############################\n",
    "def automate_result(df='bully_data_clean_with_stopword',sampling='original',sampling_ratio=1):\n",
    "    \n",
    "    ####################\n",
    "    # Reset Processing #\n",
    "    ####################\n",
    "    # first check whether file exists or not\n",
    "    # calling remove method to delete the csv file\n",
    "    # in remove method you need to pass file name and type\n",
    "    \n",
    "    task = 'bully_binary_classification'\n",
    "    file = task + '/' + df + '/results/results_' + sampling + '_sample.csv'\n",
    "    #file = df + '/results/results_all.csv'\n",
    "    if(os.path.exists(file) and os.path.isfile(file)):\n",
    "        os.remove(file)\n",
    "        print(\"File deleted\")\n",
    "    else:\n",
    "        print(\"File cleared\")\n",
    "     \n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # Train and Test Model #\n",
    "    ########################\n",
    "    # Note    \n",
    "    # classifier_name and pipeline\n",
    "    # feature_name and X\n",
    "    \n",
    "    def run_model(classifier_name, feature_name, splits, X, Y, pipeline, average_method,target_label):\n",
    "        \n",
    "        # Instantiate \n",
    "        # kfold = StratifiedShuffleSplit(n_splits=splits, test_size=0.1, random_state=1127)\n",
    "        kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1127)\n",
    "        accuracy = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "        auc = []\n",
    "        \n",
    "        record_cols = [\"sampling_method\",\"classifier\",\"feature\",\n",
    "                       \"accuracy\",\"accuracy_std\",\n",
    "                       \"precision\",\"precision_std\",\n",
    "                       \"recall\",\"recall_std\",\n",
    "                       \"f1\",\"f1_std\",\n",
    "                       \"auc\",\"auc_std\"]\n",
    "                \n",
    "             \n",
    "        # Run cross-validation\n",
    "        print(\"[\"+ sampling + \", \" + classifier_name + \"] Developing Model and Generating Metrics for features: \" + feature_name)\n",
    "        for train, test in tqdm(kfold.split(X, Y)):\n",
    "\n",
    "            # Train and fit model\n",
    "            model_fit = pipeline.fit(X[train], Y[train])\n",
    "            prediction = model_fit.predict(X[test])\n",
    "\n",
    "            # Compute metrics\n",
    "            scores = model_fit.score(X[test],Y[test])\n",
    "            accuracy.append(scores * 100)\n",
    "            if target_label == None:\n",
    "                precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n",
    "                recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n",
    "                f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n",
    "                \n",
    "                if classifier_name == \"RandomForest\":\n",
    "                    auc.append(roc_auc_score(Y[test], model_fit.predict_proba(X[test])[:,1], average=None)*100)  \n",
    "                else:\n",
    "                    auc.append(roc_auc_score(Y[test], model_fit.decision_function(X[test]), average=None)*100)\n",
    "            else:\n",
    "                precision.append(precision_score(Y[test], prediction, average=average_method, pos_label=target_label)*100)\n",
    "                recall.append(recall_score(Y[test], prediction, average=average_method, pos_label=target_label)*100)\n",
    "                f1.append(f1_score(Y[test], prediction, average=average_method, pos_label=target_label)*100)\n",
    "                if classifier_name == \"RandomForest\":\n",
    "                    auc.append(roc_auc_score(Y[test], model_fit.predict_proba(X[test])[:,1], average=None)*100)  \n",
    "                else:\n",
    "                    auc.append(roc_auc_score(Y[test], model_fit.decision_function(X[test]), average=None)*100)\n",
    "    \n",
    "                \n",
    "        record = zip([sampling],\n",
    "                     [classifier_name], [feature_name],\n",
    "                     [np.mean(accuracy)], [np.std(accuracy)],\n",
    "                     [np.mean(precision)], [np.std(precision)] ,               \n",
    "                     [np.mean(recall)], [np.std(recall)] ,               \n",
    "                     [np.mean(f1)], [np.std(f1)],               \n",
    "                     [np.mean(auc)], [np.std(auc)]                \n",
    "                    ) \n",
    "\n",
    "        df = pd.DataFrame(record, columns=record_cols)\n",
    "        \n",
    "        df.to_csv(file,mode='a', header=(index==0))\n",
    "\n",
    "        \n",
    "        \n",
    "    #########################\n",
    "    # Classifier Dictionary #\n",
    "    #########################\n",
    "\n",
    "    classifier_dict = { 'LogisticRegression': lr,\n",
    "                        'LibSVC': svc\n",
    "                        }\n",
    "    \n",
    "        \n",
    "    \n",
    "    #######################\n",
    "    # Features Dictionary #\n",
    "    #######################\n",
    "    # Load Pickle files for X feature vectors\n",
    "    \n",
    "    path = task + '\\\\' + df + '\\\\features\\\\selected'\n",
    "    all_files = glob.glob(path + \"/X*.pkl\")\n",
    "    feature_dict = {}\n",
    "\n",
    "    for file_ in all_files:\n",
    "        # temp = file_.split('\\\\')[-1].split('.')[0]\n",
    "        temp = file_.split('\\\\')[-1].split('.')[0].split(\"_\")[-1] # e.g X_AllTextual.pkl\n",
    "        \n",
    "        with open(file_,'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            feature_dict[temp]  = x\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################\n",
    "    # Target Label #\n",
    "    ################\n",
    "    # Load Pickle file for Y label\n",
    "    \n",
    "    with open(task + '\\\\' + df + '\\\\target_class\\\\Y_cyberbullying.pkl','rb') as f:\n",
    "        Y_label = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # Run through the loop #\n",
    "    ########################\n",
    "   \n",
    "    index = 0 # Initialization\n",
    "    for classifier in classifier_dict.keys():\n",
    "        \n",
    "        # Selection of Pipeline by sampling method \n",
    "        if sampling == \"original\":\n",
    "            selected_pipeline = Pipeline([ \n",
    "                                          ('scaler',scaler),\n",
    "                                          ('classifier', classifier_dict[classifier])])\n",
    "\n",
    "        elif sampling == \"oversampling\":\n",
    "            selected_pipeline =  make_pipeline(scaler,\n",
    "                                               RandomOverSampler(random_state=1127,sampling_strategy=sampling_ratio),\n",
    "                                               classifier_dict[classifier])\n",
    "            \n",
    "        elif sampling == \"smote\":\n",
    "            selected_pipeline =  make_pipeline(scaler,\n",
    "                                               SMOTE(random_state=1127,sampling_strategy=sampling_ratio),\n",
    "                                               classifier_dict[classifier])     \n",
    "        elif sampling == \"downsampling\":\n",
    "            selected_pipeline = make_pipeline(scaler,\n",
    "                                              RandomUnderSampler(random_state=1127,sampling_strategy=sampling_ratio),\n",
    "                                              classifier_dict[classifier])\n",
    "\n",
    "        for feature in tqdm(feature_dict.keys()):\n",
    "            X_feature = feature_dict[feature]\n",
    "            run_model(classifier_name=classifier, \n",
    "                      feature_name=feature, \n",
    "                      splits=10, \n",
    "                      X=X_feature, \n",
    "                      Y=Y_label, \n",
    "                      pipeline = selected_pipeline, \n",
    "                      average_method = 'binary', # macro for multiclass, binary for binary classification\n",
    "                      target_label = 'Cyberbullying') # Specify Cyberbullying for binary classification\n",
    "            index = index + 1\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ee6845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original, LogisticRegression] Developing Model and Generating Metrics for features: TermListsRationew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:06,  6.80s/it]\u001b[A\n",
      "2it [00:09,  4.18s/it]\u001b[A\n",
      "3it [00:11,  3.30s/it]\u001b[A\n",
      "4it [00:13,  2.94s/it]\u001b[A\n",
      "5it [00:16,  2.79s/it]\u001b[A\n",
      "6it [00:18,  2.58s/it]\u001b[A\n",
      "7it [00:20,  2.43s/it]\u001b[A\n",
      "8it [00:22,  2.37s/it]\u001b[A\n",
      "9it [00:25,  2.34s/it]\u001b[A\n",
      "10it [00:27,  2.74s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original, LibSVC] Developing Model and Generating Metrics for features: TermListsRationew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.48s/it]\u001b[A\n",
      "2it [00:02,  1.35s/it]\u001b[A\n",
      "3it [00:03,  1.28s/it]\u001b[A\n",
      "4it [00:05,  1.24s/it]\u001b[A\n",
      "5it [00:06,  1.26s/it]\u001b[A\n",
      "6it [00:07,  1.22s/it]\u001b[A\n",
      "7it [00:08,  1.22s/it]\u001b[A\n",
      "8it [00:10,  1.25s/it]\u001b[A\n",
      "9it [00:11,  1.27s/it]\u001b[A\n",
      "10it [00:12,  1.27s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Data 1: 'bully_data_clean_with_stopword'\n",
    "###########################################\n",
    "\n",
    "automate_result(df='bully_data_clean_with_stopword',sampling='original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e06325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smote, LogisticRegression] Developing Model and Generating Metrics for features: TermListsRationew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.58s/it]\u001b[A\n",
      "2it [00:07,  3.58s/it]\u001b[A\n",
      "3it [00:10,  3.64s/it]\u001b[A\n",
      "4it [00:14,  3.57s/it]\u001b[A\n",
      "5it [00:17,  3.53s/it]\u001b[A\n",
      "6it [00:21,  3.45s/it]\u001b[A\n",
      "7it [00:23,  3.13s/it]\u001b[A\n",
      "8it [00:25,  2.90s/it]\u001b[A\n",
      "9it [00:28,  2.76s/it]\u001b[A\n",
      "10it [00:30,  3.10s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:30<00:00, 30.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smote, LibSVC] Developing Model and Generating Metrics for features: TermListsRationew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.86s/it]\u001b[A\n",
      "2it [00:05,  2.74s/it]\u001b[A\n",
      "3it [00:08,  2.86s/it]\u001b[A\n",
      "4it [00:11,  2.84s/it]\u001b[A\n",
      "5it [00:14,  2.94s/it]\u001b[A\n",
      "6it [00:17,  2.89s/it]\u001b[A\n",
      "7it [00:20,  2.92s/it]\u001b[A\n",
      "8it [00:22,  2.80s/it]\u001b[A\n",
      "9it [00:25,  2.83s/it]\u001b[A\n",
      "10it [00:28,  2.84s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:28<00:00, 28.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "automate_result(df='bully_data_clean_with_stopword',sampling='smote',sampling_ratio=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c427f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
